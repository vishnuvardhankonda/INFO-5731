{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Wednesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c58ec175-8e26-4ad2-f9b7-2904b90649e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 1000 reviews to 'imdb_reviews_output.csv'.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def fetch_imdb_reviews(movie_links):\n",
        "    aggregated_reviews = []\n",
        "    for movie_link in movie_links:\n",
        "        page_counter = 0\n",
        "        while len(aggregated_reviews) < 1000:\n",
        "            constructed_url = f\"{movie_link[0]}?sort=userRating&dir=desc&ratingFilter=0&start={page_counter * 25}\"\n",
        "            response = requests.get(constructed_url)\n",
        "            if response.ok:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                section_reviews = soup.select('.lister-list .review-container')\n",
        "                if not section_reviews:\n",
        "                    print(f\"No more reviews found for URL: {constructed_url}\")\n",
        "                    break\n",
        "                for review in section_reviews:\n",
        "                    text_section = review.select_one('.text.show-more__control')\n",
        "                    review_text = text_section.text if text_section else \"No review text.\"\n",
        "                    rating_section = review.select_one('.rating-other-user-rating span')\n",
        "                    review_rating = rating_section.text if rating_section else \"No rating.\"\n",
        "                    aggregated_reviews.append([review_text, review_rating])\n",
        "                    if len(aggregated_reviews) == 1000:\n",
        "                        break\n",
        "                page_counter += 1\n",
        "            else:\n",
        "                print(f\"Error retrieving reviews from {constructed_url}\")\n",
        "                break\n",
        "        if len(aggregated_reviews) >= 1000:\n",
        "            break\n",
        "    return aggregated_reviews\n",
        "\n",
        "def saved_reviews(reviews, file_name):\n",
        "    headers = ['Review Text', 'Rating']\n",
        "    with open(file_name, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(headers)\n",
        "        for review in reviews:\n",
        "            writer.writerow(review)\n",
        "\n",
        "def main():\n",
        "    movie_url = [\n",
        "        ('https://www.imdb.com/title/tt1462764/reviews',)\n",
        "    ]\n",
        "    reviews = fetch_imdb_reviews(movie_url)\n",
        "    saved_reviews(reviews, 'imdb_reviews_output.csv')\n",
        "    print(f\"Saved {len(reviews)} reviews to 'imdb_reviews_output.csv'.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f062480-f3dd-4bd9-dad0-b08cf5fe9537"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished cleaning and saved the cleaned data to 'imdb_reviews_cleaned.csv'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "df = pd.read_csv('imdb_reviews_output.csv')\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
        "    words = [word.lower() for word in words]\n",
        "    stemmer = PorterStemmer()\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    cleaned_data = ' '.join(words)\n",
        "    return cleaned_data\n",
        "\n",
        "df['Cleaned Review Text'] = df['Review Text'].apply(clean_text)\n",
        "\n",
        "df.to_csv('imdb_reviews_cleaned.csv', index=False)\n",
        "\n",
        "print(\"Finished cleaning and saved the cleaned data to 'imdb_reviews_cleaned.csv'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a0aa564-4235-47c1-8d31-dcd3213ca7eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example Text: movi pan critic loss element weve come expect indi indcred breathtak action sequenc car chase hors full gallop mysteri artifact memor bad guy question good guy andharrison ford year old still fantast job action hero trhow nowexpect work luca spielberg ahv lavishli produc beauti shot piec cinemat entertain nobl prizewin literatur win award academi exactli suppos give u hour highli engag entertain valu money top bring harrison ford tenur indi satisfi conclus hold true origin portray main charact\n",
            "\n",
            "POS Tag Counts: Counter({'PROPN': 33, 'NOUN': 26, 'VERB': 9, 'ADJ': 8, 'PRON': 1, 'AUX': 1, 'ADV': 1})\n",
            "\n",
            "Dependency Parsing (first sentence):\n",
            "movi -> compound -> element\n",
            "pan -> compound -> element\n",
            "critic -> compound -> element\n",
            "loss -> compound -> element\n",
            "element -> nsubj -> expect\n",
            "we -> nsubj -> come\n",
            "ve -> aux -> come\n",
            "come -> relcl -> element\n",
            "expect -> ROOT -> expect\n",
            "indi -> nsubj -> indcred\n",
            "indcred -> ccomp -> expect\n",
            "breathtak -> nmod -> hors\n",
            "action -> compound -> sequenc\n",
            "sequenc -> compound -> chase\n",
            "car -> compound -> chase\n",
            "chase -> compound -> hors\n",
            "hors -> dobj -> indcred\n",
            "full -> amod -> guy\n",
            "gallop -> nmod -> memor\n",
            "mysteri -> amod -> memor\n",
            "artifact -> compound -> memor\n",
            "memor -> npadvmod -> bad\n",
            "bad -> amod -> guy\n",
            "guy -> compound -> question\n",
            "question -> conj -> indcred\n",
            "good -> amod -> guy\n",
            "guy -> compound -> andharrison\n",
            "andharrison -> compound -> year\n",
            "ford -> compound -> year\n",
            "year -> npadvmod -> old\n",
            "old -> advcl -> indcred\n",
            "still -> advmod -> fantast\n",
            "fantast -> compound -> nowexpect\n",
            "job -> compound -> hero\n",
            "action -> compound -> hero\n",
            "hero -> compound -> trhow\n",
            "trhow -> compound -> nowexpect\n",
            "nowexpect -> amod -> beauti\n",
            "work -> compound -> luca\n",
            "luca -> compound -> spielberg\n",
            "spielberg -> compound -> lavishli\n",
            "ahv -> compound -> lavishli\n",
            "lavishli -> compound -> beauti\n",
            "produc -> compound -> beauti\n",
            "beauti -> nsubj -> shot\n",
            "shot -> ccomp -> expect\n",
            "piec -> compound -> entertain\n",
            "cinemat -> compound -> entertain\n",
            "entertain -> compound -> award\n",
            "nobl -> compound -> award\n",
            "prizewin -> compound -> award\n",
            "literatur -> compound -> win\n",
            "win -> compound -> award\n",
            "award -> compound -> suppos\n",
            "academi -> compound -> exactli\n",
            "exactli -> compound -> suppos\n",
            "suppos -> nsubj -> give\n",
            "give -> ccomp -> expect\n",
            "u -> dative -> give\n",
            "hour -> compound -> entertain\n",
            "highli -> compound -> engag\n",
            "engag -> compound -> entertain\n",
            "entertain -> dobj -> give\n",
            "valu -> ccomp -> expect\n",
            "money -> compound -> top\n",
            "top -> dobj -> valu\n",
            "bring -> ccomp -> expect\n",
            "harrison -> compound -> tenur\n",
            "ford -> compound -> tenur\n",
            "tenur -> dobj -> bring\n",
            "indi -> compound -> conclus\n",
            "satisfi -> compound -> conclus\n",
            "conclus -> nsubj -> hold\n",
            "hold -> ccomp -> expect\n",
            "true -> amod -> origin\n",
            "origin -> nsubj -> portray\n",
            "portray -> ccomp -> hold\n",
            "main -> amod -> charact\n",
            "charact -> dobj -> hold\n",
            "\n",
            "Named Entity Counts: Counter({'GPE': 2080, 'ORG': 1720, 'PERSON': 1720, 'CARDINAL': 640, 'DATE': 600, 'NORP': 560, 'ORDINAL': 440, 'TIME': 40, 'PRODUCT': 40, 'EVENT': 40})\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "df = pd.read_csv('imdb_reviews_cleaned.csv')\n",
        "\n",
        "def parts_of_speech_tagging(text):\n",
        "    doc = nlp(text)\n",
        "    parts_of_speech_count = Counter(token.pos_ for token in doc)\n",
        "    return parts_of_speech_count\n",
        "\n",
        "def constituency_dependency_parsing(text):\n",
        "    doc = nlp(text)\n",
        "    for token in doc:\n",
        "        print(f\"{token.text} -> {token.dep_} -> {token.head.text}\")\n",
        "\n",
        "def named_entity_recognition(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [ent.label_ for ent in doc.ents]\n",
        "    return Counter(entities)\n",
        "\n",
        "\n",
        "example_text = df['Cleaned Review Text'].iloc[0]\n",
        "print(f\"Example Text: {example_text}\\n\")\n",
        "\n",
        "parts_of_speech_count = parts_of_speech_tagging(example_text)\n",
        "print(\"POS Tag Counts:\", parts_of_speech_count)\n",
        "\n",
        "print(\"\\nDependency Parsing (first sentence):\")\n",
        "constituency_dependency_parsing(example_text.split('.')[0])\n",
        "\n",
        "entity_counts = named_entity_recognition(' '.join(df['Cleaned Review Text']))\n",
        "print(\"\\nNamed Entity Counts:\", entity_counts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"Fetching 1000 reviews from imdb is really challenging, using chatgpt and understanding the logic behind I kind of learn debugging as well.\""
      ],
      "metadata": {
        "id": "_e557s2w4BpK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "cf46a029-1239-4f47-b47a-56c0aa2c3fbd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Fetching 1000 reviews from imdb is really challenging, using chatgpt and understanding the logic behind I kind of learn debugging as well.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OP4-5qxAtInv"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}