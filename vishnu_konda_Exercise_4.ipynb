{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 4**\n",
        "\n",
        "**This exercise will provide a valuable learning experience in working with text data and extracting features using various topic modeling algorithms. Key concepts such as Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA), lda2vec, and BERTopic.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "\n",
        "**Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afbef547-e533-4ff1-ffbb-717e3ce5009f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Number of Topics: 14\n",
            "Topic 8: ['effects', 'water', 'communities', 'scarcity', 'conservation', 'impact', 'climate', 'environment', 'deforestation', 'urban']\n",
            "Topic 13: ['change', 'climate', 'role', 'policy', 'mitigating', 'global', 'patterns', 'effects', 'weather', 'environmental']\n",
            "Topic 3: ['impact', 'climate', 'environment', 'conservation', 'communities', 'scarcity', 'food', 'effects', 'importance', 'security']\n",
            "Topic 11: ['impact', 'climate', 'scarcity', 'deforestation', 'urban', 'environment', 'conservation', 'change', 'effects', 'planning']\n",
            "Topic 7: ['biodiversity', 'ecosystem', 'conservation', 'importance', 'climate', 'scarcity', 'environment', 'impact', 'food', 'deforestation']\n",
            "Topic 1: ['ocean', 'management', 'protection', 'strategies', 'conservation', 'pollution', 'control', 'marine', 'waste', 'life']\n",
            "Topic 12: ['impact', 'food', 'climate', 'environment', 'conservation', 'deforestation', 'scarcity', 'change', 'effects', 'importance']\n",
            "Topic 10: ['effects', 'climate', 'environment', 'impact', 'scarcity', 'conservation', 'deforestation', 'food', 'importance', 'water']\n",
            "Topic 6: ['impact', 'environment', 'climate', 'conservation', 'effects', 'scarcity', 'agriculture', 'importance', 'green', 'change']\n",
            "Topic 5: ['environment', 'impact', 'climate', 'importance', 'conservation', 'scarcity', 'food', 'effects', 'communities', 'deforestation']\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    \"Climate change and its effects on global weather patterns\",\n",
        "    \"Renewable energy sources and their impact on sustainability\",\n",
        "    \"The importance of biodiversity and ecosystem conservation\",\n",
        "    \"Pollution control and waste management strategies\",\n",
        "    \"Deforestation and its impact on the environment\",\n",
        "    \"Sustainable agriculture and food security\",\n",
        "    \"Water scarcity and its effects on communities\",\n",
        "    \"The role of environmental policy in mitigating climate change\",\n",
        "    \"Urban planning and green infrastructure\",\n",
        "    \"Ocean conservation and marine life protection\"\n",
        "]\n",
        "\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess(texts):\n",
        "    stop_words = stopwords.words('english')\n",
        "    return [[word for word in simple_preprocess(doc) if word not in stop_words] for doc in texts]\n",
        "\n",
        "processed_data = preprocess(data)\n",
        "\n",
        "# Create Dictionary and Corpus\n",
        "id2word = corpora.Dictionary(processed_data)\n",
        "corpus = [id2word.doc2bow(text) for text in processed_data]\n",
        "\n",
        "# Function to compute coherence values for different number of topics\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = gensim.models.LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=dictionary, passes=10, workers=2)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return model_list, coherence_values\n",
        "\n",
        "# Compute coherence values for different numbers of topics\n",
        "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=processed_data, limit=20)\n",
        "\n",
        "# Select the model with the highest coherence and print the topics\n",
        "optimal_model = model_list[coherence_values.index(max(coherence_values))]\n",
        "optimal_num_topics = optimal_model.num_topics\n",
        "topics = optimal_model.show_topics(num_words=10, formatted=False)\n",
        "\n",
        "print(f\"Optimal Number of Topics: {optimal_num_topics}\")\n",
        "for topic in topics:\n",
        "    print(f\"Topic {topic[0]}: {[word[0] for word in topic[1]]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "\n",
        "**Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53afc826-569c-4a0f-e6d8-5ac11bb9397f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Number of Topics: 7\n",
            "Topics:\n",
            "(0, '-0.605*\"data\" + -0.347*\"technology\" + -0.227*\"blockchain\" + -0.227*\"use\" + -0.227*\"securing\" + -0.189*\"learning\" + -0.189*\"machine\" + -0.189*\"role\" + -0.189*\"analysis\" + -0.189*\"big\"')\n",
            "(1, '-0.740*\"reality\" + -0.370*\"future\" + -0.370*\"augmented\" + -0.370*\"virtual\" + 0.078*\"connecting\" + 0.078*\"physical\" + 0.078*\":\" + 0.078*\"digital\" + 0.078*\"things\" + 0.078*\"internet\"')\n",
            "(2, '-0.370*\":\" + -0.370*\"worlds\" + -0.370*\"connecting\" + -0.370*\"internet\" + -0.370*\"physical\" + -0.370*\"things\" + -0.370*\"digital\" + -0.155*\"reality\" + -0.078*\"virtual\" + -0.078*\"augmented\"')\n",
            "(3, '-0.498*\"impact\" + -0.355*\"technology\" + -0.315*\"evolution\" + -0.315*\"mobile\" + -0.315*\"society\" + 0.245*\"data\" + -0.184*\"artificial\" + -0.184*\"healthcare\" + -0.184*\"intelligence\" + 0.143*\"significance\"')\n",
            "(4, '0.632*\"computing\" + 0.316*\"innovations\" + 0.316*\"trends\" + 0.316*\"cloud\" + 0.316*\"quantum\" + 0.316*\"advances\" + 0.316*\"applications\" + 0.000*\"reality\" + 0.000*\"augmented\" + 0.000*\"virtual\"')\n",
            "(5, '0.377*\"role\" + 0.377*\"learning\" + 0.377*\"machine\" + 0.377*\"analysis\" + -0.248*\"business\" + -0.248*\"big\" + -0.248*\"significance\" + -0.248*\"modern\" + -0.129*\"blockchain\" + -0.129*\"securing\"')\n",
            "(6, '-0.303*\"threats\" + -0.303*\"prevention\" + -0.303*\"measures\" + -0.303*\"cybersecurity\" + -0.264*\"significance\" + -0.264*\"business\" + -0.264*\"modern\" + -0.264*\"big\" + 0.206*\"use\" + 0.206*\"securing\"')\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim import corpora, models\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download and set up necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Sample dataset\n",
        "data = [\n",
        "    \"Advances in quantum computing and its applications\",\n",
        "    \"The impact of artificial intelligence on healthcare\",\n",
        "    \"Blockchain technology and its use in securing data\",\n",
        "    \"The future of augmented reality and virtual reality\",\n",
        "    \"Cybersecurity threats and prevention measures\",\n",
        "    \"The role of machine learning in data analysis\",\n",
        "    \"Internet of Things: Connecting the physical and digital worlds\",\n",
        "    \"The evolution of mobile technology and its impact on society\",\n",
        "    \"Cloud computing trends and innovations\",\n",
        "    \"The significance of big data in modern business\"\n",
        "]\n",
        "# Preprocess the data\n",
        "stop_words = set(stopwords.words('english'))\n",
        "texts = [[word for word in word_tokenize(document.lower()) if word not in stop_words] for document in data]\n",
        "\n",
        "# Create the Document-Term Matrix\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Function to compute coherence values for different number of topics\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=1):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = models.LsiModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return model_list, coherence_values\n",
        "\n",
        "# Set the range for number of topics\n",
        "start, limit, step = 2, 10, 1\n",
        "\n",
        "# Compute coherence values for different numbers of topics\n",
        "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=texts, start=start, limit=limit, step=step)\n",
        "\n",
        "# Find the optimal model\n",
        "max_coherence_value = max(coherence_values)\n",
        "optimal_model_index = coherence_values.index(max_coherence_value)\n",
        "optimal_model = model_list[optimal_model_index]\n",
        "optimal_num_topics = start + optimal_model_index\n",
        "\n",
        "# Summarize the topics\n",
        "topics = optimal_model.show_topics(num_topics=optimal_num_topics)\n",
        "\n",
        "print(f\"Optimal Number of Topics: {optimal_num_topics}\")\n",
        "print(\"Topics:\")\n",
        "for topic in topics:\n",
        "    print(topic)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "**Generate K topics by using lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4afc35e1-246a-48d3-f82d-a30a084b1b2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "# Convert to lowercase\n",
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "# Tokenize\n",
        "def tokenize(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Remove stop words\n",
        "def remove_stopwords(texts):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [[word for word in text if word not in stop_words] for text in texts]\n",
        "\n",
        "# Lemmatize\n",
        "def lemmatize(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
        "    lemmatized_texts = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent))\n",
        "        lemmatized_sent = [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n",
        "        lemmatized_texts.append(lemmatized_sent)\n",
        "    return lemmatized_texts\n",
        "\n",
        "# Create bigrams\n",
        "def create_bigrams(texts, id2word):\n",
        "    bigram = gensim.models.Phrases(texts, min_count=5, threshold=100)\n",
        "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "    return [bigram_mod[doc] for doc in texts], id2word\n",
        "\n",
        "# Create trigrams\n",
        "def create_trigrams(texts, id2word):\n",
        "    bigram = gensim.models.Phrases(texts, min_count=5, threshold=100)\n",
        "    trigram = gensim.models.Phrases(bigram[texts], threshold=100)\n",
        "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts], id2word\n",
        "\n",
        "# Preprocess text data\n",
        "def preprocess_text_data(data, id2word):\n",
        "    data = remove_punctuation(data)\n",
        "    data = to_lowercase(data)\n",
        "    data = tokenize(data)\n",
        "    data = remove_stopwords(data)\n",
        "    data = lemmatize(data)\n",
        "    return [id2word.doc2bow(text) for text in data], id2word\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "**Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1c5dcd4-aa61-46e4-9a0d-cb25f3a6afaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Topic  Count                         Name  \\\n",
            "0     -1    598             -1_the_of_to_and   \n",
            "1      0    231              0_the_to_of_and   \n",
            "2      1    107           1_image_and_for_of   \n",
            "3      2     89          2_the_points_den_is   \n",
            "4      3     87       3_card_vesa_mode_video   \n",
            "5      4     62           4_for_and_data_the   \n",
            "6      5     56            5_sky_the_that_to   \n",
            "7      6     55          6_jpeg_gif_you_file   \n",
            "8      7     51  7_conference_int_nok_oprows   \n",
            "9      8     49    8_hst_the_reboost_mission   \n",
            "\n",
            "                                      Representation  \\\n",
            "0      [the, of, to, and, in, is, it, that, for, on]   \n",
            "1   [the, to, of, and, that, space, in, is, be, for]   \n",
            "2   [image, and, for, of, or, it, 3d, is, the, with]   \n",
            "3  [the, points, den, is, of, problem, this, line...   \n",
            "4  [card, vesa, mode, video, the, to, it, with, p...   \n",
            "5  [for, and, data, the, ftp, in, available, is, ...   \n",
            "6  [sky, the, that, to, advertising, of, it, righ...   \n",
            "7  [jpeg, gif, you, file, image, is, format, if, ...   \n",
            "8  [conference, int, nok, oprows, opcols, for, an...   \n",
            "9  [hst, the, reboost, mission, to, they, shuttle...   \n",
            "\n",
            "                                 Representative_Docs  \n",
            "0  [LARSONIAN Astronomy and Physics\\n\\n          ...  \n",
            "1  [\\nYou totally forgot the original post that y...  \n",
            "2  [\\n: >Hi Netters,\\n: >\\n: >I'm building a CAD ...  \n",
            "3  [I'm asking for help on a sticky problem invol...  \n",
            "4  [Hello\\n\\tI have posted to this newsgroup once...  \n",
            "5  [Archive-name: graphics/faq\\n\\nThis message is...  \n",
            "6  [Two developments have brought these type of a...  \n",
            "7  [Archive-name: jpeg-faq\\nLast-modified: 16 May...  \n",
            "8  [Greetings this is a general call for informat...  \n",
            "9  [\\nI hate to belabor the obvious once again, b...  \n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from bertopic import BERTopic\n",
        "\n",
        "# Load the dataset with specified categories\n",
        "categories = ['sci.space', 'comp.graphics']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "texts = newsgroups.data\n",
        "\n",
        "# Initialize and fit BERTopic\n",
        "topic_model = BERTopic()\n",
        "topics, _ = topic_model.fit_transform(texts)\n",
        "\n",
        "# Print the top 10 topics\n",
        "print(topic_model.get_topic_info().head(10))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Question (5 Points)\n",
        "\n",
        "**Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details.**\n",
        "\n",
        "**This question will compensate for any points deducted in this exercise. Maximum marks for the exercise is 40 points.**"
      ],
      "metadata": {
        "id": "d89ODUx3jjJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LDA (Latent Dirichlet Allocation)\n",
        "# Preferred for huge datasets with well-defined and unique subjects due to its simplicity and interpretability.\n",
        "# Example usage:\n",
        "# from sklearn.decomposition import LatentDirichletAllocation\n",
        "# lda_model = LatentDirichletAllocation(n_components=number_of_topics)\n",
        "# lda_topic_matrix = lda_model.fit_transform(document_term_matrix)\n",
        "\n",
        "# LSA (Latent Semantic Analysis)\n",
        "# Appropriate for jobs requiring dimensionality reduction and computational efficiency, particularly in singular value decomposition.\n",
        "# Example usage:\n",
        "# from sklearn.decomposition import TruncatedSVD\n",
        "# lsa_model = TruncatedSVD(n_components=number_of_topics)\n",
        "# lsa_topic_matrix = lsa_model.fit_transform(document_term_matrix)\n",
        "\n",
        "# lda2vec\n",
        "# Excellent for capturing semantic and topical linkages in texts, requires more computer resources and a larger dataset.\n",
        "# Note: lda2vec is not directly available in sklearn, needs custom implementation or other libraries.\n",
        "\n",
        "# BERTopic\n",
        "# Perfect for extracting nuanced and contextually rich topics from complicated text input, but requires substantial computational capacity.\n",
        "# Example usage:\n",
        "# from bertopic import BERTopic\n",
        "# bertopic_model = BERTopic()\n",
        "# topics, probabilities = bertopic_model.fit_transform(docs)\n"
      ],
      "metadata": {
        "id": "OK34nZtojhmm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "Consider the following points in your response:\n",
        "\n",
        "**Learning Experience:** Describe your overall learning experience in working with text data and extracting features using various topic modeling algorithms. Did you understand these algorithms and did the implementations helped in grasping the nuances of feature extraction from text data.\n",
        "\n",
        "**Challenges Encountered:** Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "#The process of experimenting with various topic modeling methods such as LDA, LSA, lda2vec, and BERTopic gives a full learning experience in text data analysis. Understanding these techniques is critical to comprehending the complexities of feature extraction from text data\n",
        "#Interpreting the themes and their importance can be subjective and sometimes difficult, especially when the issues are not easily distinct or are too detailed.\n",
        "#Understanding and implementing these methods improves one's capacity to efficiently handle and evaluate vast amounts of text data, which is an important skill in the NLP sector. This exercise combines theoretical understanding and practical application, offering a good basis for further investigation into complex NLP problems.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "7a32f683-4756-428b-c4a3-bf226b4b2954"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nPlease write you answer here:\\n#The process of experimenting with various topic modeling methods such as LDA, LSA, lda2vec, and BERTopic gives a full learning experience in text data analysis. Understanding these techniques is critical to comprehending the complexities of feature extraction from text data\\n#Interpreting the themes and their importance can be subjective and sometimes difficult, especially when the issues are not easily distinct or are too detailed.\\n#Understanding and implementing these methods improves one's capacity to efficiently handle and evaluate vast amounts of text data, which is an important skill in the NLP sector. This exercise combines theoretical understanding and practical application, offering a good basis for further investigation into complex NLP problems.\\n\\n\\n\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}