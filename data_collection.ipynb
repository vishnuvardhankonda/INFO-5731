{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# converting the text files to dataframe"
      ],
      "metadata": {
        "id": "Tp5Z2Nquik8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "# Directory containing your text files\n",
        "directory_path = '/content/drive/MyDrive/AllPatentTextDocs'\n",
        "\n",
        "# Initialize an empty list to store dictionaries representing each document\n",
        "data = []\n",
        "\n",
        "# Loop through each file in the directory\n",
        "for filename in os.listdir(directory_path):\n",
        "    if filename.endswith(\".txt\"):  # Process only text files\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            # Read all lines from the file\n",
        "            lines = file.readlines()\n",
        "\n",
        "            # Initialize dictionary to store data for this document\n",
        "            doc_data = {}\n",
        "\n",
        "            # Loop through each line in the file\n",
        "            for line in lines:\n",
        "                # Split the line into key and value using the first occurrence of ':' as the separator\n",
        "                key_value_pair = line.split(':', 1)\n",
        "                if len(key_value_pair) == 2:  # Ensure key and value are present\n",
        "                    key = key_value_pair[0].strip()\n",
        "                    value = key_value_pair[1].strip()\n",
        "                    doc_data[key] = value\n",
        "\n",
        "            # Append the document data to the list\n",
        "            data.append(doc_data)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "fjZtAH-VlQY8",
        "outputId": "c50710f1-e9a9-41e0-85bd-1df70653983f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-367081797f47>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# Read all lines from the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# Initialize dictionary to store data for this document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(data[1])\n",
        "type(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GreA8IRGnHlL",
        "outputId": "620c168e-1f6b-4ac6-df59-51ba740f5824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Patent Number': 'US7490152', 'Title': 'Version caching mechanism', 'PDF Link': 'https://patentimages.storage.googleapis.com/d7/48/85/fde705020148cf/US7490152B2.pdf', 'Abstract': 'A protocol identifier such as a GTP version identifier is automatically cached in association with device contact information. Setup latency is reduced for subsequent connections. For instance the cache indicates that a GGSN only supports GTPV0. An SGSN attempts to create a PDP context with the GGSN using GTPV0 without first trying GTPV1. If the cache does not include a version identifier for an APN IP address the SGSN tries to create a context with a first version (e.g. GTPV1) and if necessary a second version (e.g. GTPV0). An identifier associated with the successful protocol is stored in the cache in association with the contact information (e.g. APN or APN IP address). Freshness of cached information is assured. For example when all tunnels associated with an APN IP address are torn down a flush timer is started. If the timer expires the cache entry is removed.', 'Claims': '1. A method for caching a supported protocol of a device the method comprising:receiving a first request to establish a connection with the device the request being associated with contact information related to the device;using the contact information as a key or index into a protocol cache;if a previously stored protocol identifier is stored in the protocol cache in association with the contact information attempting to establish the connection with the device using a protocol associated with the previously stored protocol identifier;attempting to establish the connection with the device with a first protocol if the previously stored protocol identifier is not stored in the protocol cache in association with the contact information;storing a first protocol identifier in the protocol cache in association with the contact information if the attempt to establish the connection with the device with the first protocol was successful;attempting to establish the connection with the device with a second protocol if the attempt to establish the connection with the device with the first protocol was not successful; andstoring a second protocol identifier in the protocol cache in association with the contact information if the attempt to establish the connection with the device with the second protocol was successful and the attempt to establish the connection with the device with the first protocol was not successful. 2. The method for caching a supported protocol of claim 1 further comprising: removing the association of the stored first protocol identifier or second protocol identifier with the contact information upon the expiration of a flush timer. 3. The method for caching a supported protocol of claim 2 further comprising:starting the flush timer when all connections associated with the contact information between a system having the cache and the device are torn down. 4. The method for caching a supported protocol of claim 1 further comprising: removing the association of the previously stored identifier with the contact information if the attempt to establish communication with the device using the protocol associated with the previously stored protocol identifier is unsuccessful. 5. The method for caching a supported protocol of claim 1 wherein receiving the first request to establish communication with the device comprises:receiving an Activate Packet Data Protocol (PDP) Context Request message. 6. The method for caching a supported protocol of claim 1 wherein receiving the first request to establish communication with the device the request being associated with contact information related to the device comprises:receiving an Activate Packet Data Protocol (PDP) Context Request message the Activate PDP Context Request message including information identifying a requestor;accessing a database with the information identifying the requestor; and retrieving the contact information from the database. 7. The method for caching a supported protocol of claim 1 wherein receiving the first request to establish communication with the device the request being associated with contact information related to the device comprises:receiving an Activate Packet Data Protocol (PDP) Context Request message the Activate PDP Context Request message being associated with information identifying a requestor;accessing a home location register with the information identifying the requestor; andretrieving the contact information from the home location register or with information supplied by the home location register. 8. The method for caching a supported protocol of claim 1 wherein receiving the first request to establish communication with the device the request being associated with contact information related to the device comprises:receiving an Activate Packet Data Protocol (PDP) Context Request message including an APN. 9. The method for caching a supported protocol of claim 1 wherein storing the first protocol identifier or second protocol identifier in the protocol cache in association with the contact information comprises: storing a first General Packet Radio Service Tunneling Protocol (GTP) version identifier or a second General Packet Radio Service Tunneling Protocol (GTP) version identifier in a GTP version cache in association with an Access Point Name (APN) or an Access Point Name Internet Protocol (APN IP) address. 10. A method for caching a General Packet Radio Service Tunneling Protocol (GTP) version of a General Packet Radio Service (GPRS) support node the method comprising:receiving a GTP tunnel setup request message associated with an Access Point Name (APN) or an Access Point Name Internet Protocol (APN IP) address; determining if a previously stored GTP version entry associated with the APN or the APN IP address exists in a GTP version cache table;if the previously stored GTP version entry does not exist: attempting to establish a connection with the GPRS support node with a first version of GTP; determining if the attempt to establish the connection with the GPRS support node with the first version of GTP was successful;if the attempt to establish the connection with the GPRS support node with the first version of GTP was successful: storing a first GTP version identifier in the GTP version cache table in association with the APN or the APN IP address;if the attempt to establish the connection with the GPRS support node with the first version of GTP was not successful: attempting to establish the connection with the GPRS support node with a second version of GTP;determining if the attempt to establish the connection with the GPRS support node with the second version of GTP was successful;and if the attempt to establish the connection with the GPRS support node with the second version of GTP was successful: storing a second GTP version identifier in the GTP version cache table in association with the APN or the APN IP address. 11. The method for caching a GTP version of a GPRS support node of claim 10 further comprising: monitoring a status of tunnels associated with the APN or the APN IP address;starting a timer when all monitored tunnels associated with the APN or the APN IP address are torn down; and flushing the stored first GTP version identifier or second GTP version identifier cache entry associated with the APN or the APN IP address when the timer expires. 12. The method for caching a GTP version of a GPRS support node of claim 11 further comprising: removing the timer if a new tunnel is set up in association with the APN or the APN IP address before the timer expires. 13. The method for caching a GTP version of a GPRS support node of claim 10 wherein receiving the first GTP tunnel setup request message comprises: receiving a GTP Activate Packet data Protocol (PDP) Context Request message associated with an APN; and resolving the APN into a resolved APN IP address. 14. The method for caching a GTP version of a GPRS support node of claim 11 wherein monitoring the status of tunnels associated with the APN or the APN IP address comprises:incrementing a counter associated with the APN or the APN IP address each time a Packet data Protocol (PDP) context is set up in association with the APN or the APN IP address; and decrementing the counter associated with the APN or the APN IP address each time any PDP context associated with the APN or the APN IP address is torn down. 15. The method for caching a GTP version of a GPRS support node of claim 14 wherein starting the timer when all monitored tunnels associated with the APN or the APN IP address are torn down comprises: starting the timer when the counter associated with the APN or the APN IP address has a zero value. 16. The method for caching a GTP version of a GPRS support node of claim 10 further comprising:attempting to establish the connection with the GPRS support node with the version of GTP associated with the previously stored GTP version entry; andflushing the previously stored GTP version entry associated with the APN or the APN IP address if the attempt to establish the connection with the GPRS support node with the version of GTP associated with the previously stored GTP version was not successful. 17. The method for caching a GTP version of a GPRS support node of claim 16 further comprising:attempting to establish the connection with the GPRS support node with the first version of GTP;determining if the attempt to establish the connection with the GPRS support node with the first version of GTP was successful;if the attempt to establish the connection with the GPRS support node with the first version of GTP was successful:storing the first GTP version identifier in the GTP version cache table in association with the APN or the APN IP address;if the attempt to establish the connection with the GPRS support node with the first version of GTP was not successful: attempting to establish the connection with the GPRS support node with the second version of GTP;determining if the attempt to establish the connection with the GPRS support node with the second version of GTP was successful; andif the attempt to establish the connection with the GPRS support node with the second version of GTP was successful: storing the second GTP version identifier in the GTP version cache table in association with the APN or the APN IP address. 18. The method for caching a GTP version of a GPRS support node of claim 10 wherein attempting to establish the connection with the GPRS support node with the first version of GTP comprises: attempting to establish the connection with the GPRS support node with GTPv1. 19. The method for caching a GTP version of a GPRS support node of claim 10 wherein attempting to establish the connection with the GPRS support node with the second version of GTP comprises: attempting to establish the connection with the GPRS support node with GTPv0. 20. A Serving General Packet Radio Service Support Node (SGSN) comprising:a General Packet Radio Service Tunneling Protocol (GTP) version cache including a plurality of GTP version identifiers in association with a respective plurality of Access Point Names (APNs) or Access Point Name Internet Protocol (APN IP) addresses and a respective plurality of context counts;means for receiving a GTP tunnel setup request message associated with an Access Point Names (APN) or an APN IP address;means for making a first attempt to establish a connection with the General Packet Radio Service (GPRS) support node with a version of GTP associated with a previously stored GTP version identifier;means for incrementing a context count when a context is successfully set up using the APN or the APN IP address and the GTP version associated with the previously stored GTP version identifier stored in the GTP version cache in association with the APN or the APN IP address; and means for decrementing the context count when the context associated with the APN or the APN IP address and the GTP version associated with the GTP version identifier stored in the GTP version cache in association with the APN or the APN IP address is torn down. 21. The SGSN of claim 20 wherein the GTP version cache further comprises:a plurality of respective timer values in association with the respective plurality of APNs or the APN IP addresses and the respective plurality of context counts;means for updating a respective timer value according to a passage of time;means for starting the means for updating the respective timer value when a respective context count has a zero value; and means for removing an association between a respective GTP version identifier and a respective APN or APN IP address when the respective timer value reaches a predetermined value. 22. The SGSN of claim 20 further comprising: means for stopping the means for updating the respective time value before the respective time value reaches the predetermined value if the respected context count is incremented above zero value. 23. The SGSN of claim 20 further comprising: means for removing the association between the respective GTP version identifier and the respective APN or the APN IP address if the attempt to establish the connection with the GPRS support node with the version of GTP associated with the respective GTP version identifier fails. 24. The SGSN of claim 20 further comprising: means for attempting to establish the connection with the GPRS support node with a trial version of GTP; and means for storing a trial GTP version identifier in the GTP version cache in association with the APN or the APN IP address if the attempt to establish the connection with the GPRS support node with the trial version of GTP is successful. 25. The SGSN of claim 24 wherein the means for attempting to establish the connection with the GPRS support node with the trial version of GTP comprises: means for attempting to establish the connection with the GPRS support node with GTPv1. 26. The SGSN of claim 24 wherein the means for attempting to establish the connection with the GPRS support node with the trial version of GTP comprises: means for attempting to establish the connection with the GPRS support node with GTPv0. 27. A method for caching a supported protocol of a device the method comprising:receiving a first request to establish a connection with the device the request being associated with contact information related to the device;using the contact information as a key or index into a protocol cache;making a first attempt to establish the connection with the device using a protocol associated with a previously stored protocol identifier if the previously stored protocol identifier is stored in the protocol cache in association with the contact information;attempting to establish the connection with the device with a first protocol if the previously stored protocol identifier is not stored in the protocol cache in association with the contact information;storing a first protocol identifier in the protocol cache in association with the contact information if the attempt to establish the connection with the device with the first protocol was successful;attempting to establish the connection with the device with a second protocol if the attempt to establish the connection with the device with the first protocol was not successful; and storing a second protocol identifier in the protocol cache in association with the contact information if the attempt to establish the connection with the device with the second protocol was successful and the attempt to establish the connection with the device with the first protocol was not successful.', 'Classifications': \"{'H04L69/24': 'Negotiation of communication capabilities', 'H04L69/18': 'Multiprotocol handlers e.g. single devices capable of handling multiple protocols', 'H04W76/18': 'Management of setup rejection or failure', 'H04W76/12': 'Setup of transport tunnels', 'H04W84/04': 'Large scale networks; Deep hierarchical networks'}\", 'Description': '', 'BACKGROUND': 'The invention is related to the art of network interworking. The invention may find application wherever a network can include elements that communicate via different protocols. The invention will be explained in terms of different versions of a General Packet Radio Service (GPRS) tunneling protocol (GTP). However the invention may find application wherever a first device can benefit from information regarding a protocol used by a second device. There are currently two versions of the GPRS tunneling protocol (GTP). The two versions are known as GTPV0 and GTPV1. The tunneling protocols are not completely compatible. Therefore procedures have been specified that allow a first network device or platform that supports one version to communicate with another device or platform that supports the other version. However in order to interoperate with the second device or platform the first platform must determine which version is supported by the second device. In order to make this determination the 3rd Generation Partnership Project (3GPP in its technical specification 29.060) suggests that the first platform attempt to communicate with the second platform using the latest version (e.g. GTPV1). If that attempt fails for example due to an unreliable (i.e.; unacknowledged) nature of a transport protocol (e.g. UDP) the specification recommends trying to communicate with the second device using an older version (e.g. GTPV0). This procedure can introduce delays in a call setup procedure. For example each attempt to communicate is associated with a wait or response time. Each wait or response time can be on the order of 5 or 10 seconds. Generally each attempt would be associated with several retries (e.g. 3). Therefore GTP tunnel setup times of nearly a minute might not be uncommon. The 3GPP specification (29.060) allows for GTP version caching. However the specification does not specify or suggest a method for caching GTP versions. One method of keeping track of the GTP versions supported by each platform in a network is to have a database manually updated each time a platform is added to the network and each time a network component is upgraded. However such a method is time consuming expensive and prone to error. Therefore there is a desire for a method of automatically caching protocol versions so that tunnels can be set up without first making several attempts to set up a tunnel to a remote platform using an incompatible version or protocol.', 'SUMMARY': 'A method for caching a supported protocol of a remote device includes receiving a first request to establish a connection with the device the request being associated with contact information related to the device. For example the contact information can be included in the request or can be determined from information in the request such as for example an identification associated with the requestor. The method further includes using the contact information as a key or index into a protocol cache attempting to establish the connection with the device using a protocol associated with a previously stored protocol identifier if the previously stored protocol identifier is stored in association with the contact information attempting to establish the connection with the device with a first protocol if the previously stored protocol identifier is not stored in association with the contact information storing a first protocol identifier in a protocol cache in association with the contact information if the attempt to establish the connection with the device with the first protocol was successful attempting to establish a connection with the device with a second protocol if the attempt to establish the connection with the device with the first protocol was not successful and storing a second protocol identifier in the protocol cache in association with the contact information if the attempt to establish the connection with the device with the second protocol was successful and the attempt to establish the connection with the device with the first protocol was not successful. The method can further include removing the association of the previously stored first or second protocol identifier with the contact information upon the expiration of a flush timer. The flush timer can be started when all connections associated with the contact information between the system having the cache and the remote device are torn down. Alternatively or additionally the association of the previously stored identifier with the contact information can be removed if the attempt to establish communication with the device using a protocol associated with the previously stored protocol identifier is unsuccessful. In a GTP environment receiving a request to establish communication with the device can include receiving an Activate PDP Context Request message. The Activate PDP Context Request message is associated with information identifying a requester. The information identifying the requestor associates the Activate PDP Context Request message with contact information because the information identifying the requestor can be used to access a database and retrieve the contact information or information useful in determining the contact information. For example the database can be a Home Location Register (HLR). Alternatively the Activate PDP Context Request message includes contact information directly. For example the contact information is an Access Point Name (APN) (logical name) and storing the first or second protocol identifier in the protocol cache in association with the contact information includes storing a first or second GTP version identifier in a GTP version cache in association with an APN. In yet a further alternative the first or second GTP version identifier is stored in the GTP version cache in association with an APN IP address. For example the APN can be resolved into an APN IP address with locally stored information or via the services of a Domain Name Server (DNS). Some embodiments include a method for caching a GTP version of a GPRS support node including receiving a GTP tunnel setup request message associated with an APN or APN IP address. As explained above the association can be direct or indirect. The method further includes determining if a previously stored GTP version entry associated with the APN or APN IP address exists in a GTP version cache table. If a previously stored GTP version entry does not exist the method includes attempting to establish a connection with the GPRS support node with a first version (e.g. the most recently released version) of GTP determining if the attempt to establish a connection with the GPRS support node with a first version of GTP was successful and if the attempt to establish a connection with the GPRS support node with a first version of GTP was successful storing a first GTP version identifier in the GTP version cache table in association with the APN or APN IP address. If the attempt to establish a connection with the GPRS support node with the first version of GTP was not successful The method includes attempting to establish a connection with the GPRS support node with a second version of GTP and determining if the attempt to establish a connection with the GPRS support node with the second version of GTP was successful. If the attempt to establish a connection with the GPRS support node with a second version of GTP was successful the method includes storing a second GTP version identifier in the GTP version cache table in association with the APN or APN IP address. The method can further include monitoring a status of connections or tunnels associated with the APN or APN IP address starting a timer when all monitored tunnels associated with the APN or APN IP address are torn down and when the timer expires flushing the previously stored first or second GTP version identifier cache entry associated with the APN or APN IP address. The method can further include removing the timer if a new tunnel is set up in association with the APN or APN IP address before the timer expires. Receiving a first GTP tunnel setup request message can include receiving a GTP Activate PDP Context Request message associated with a APN and resolving the APN into an APN IP address. Monitoring the status of tunnels associated with the APN or APN IP address can include incrementing a counter associated with the APN or APN IP address each time a PDP context is set up in association with the APN or APN IP address and decrementing the counter associated with the APN or APN IP address each time a PDP context associated with the APN or APN IP is torn down. Starting the timer when all monitored tunnels associated with the APN or APN IP address are torn down can include starting the timer when the counter associated with the APN or APN IP address has a zero value. Some embodiments include flushing the previously stored GTP version entry associated with the APN or APN IP address if the attempt to establish a connection with the GPRS support node with the version of GTP associated with the previously stored GTP version was not successful. In some of those embodiments the method includes attempting to establish the connection with the GPRS support node with the first version of GTP determining if the attempt to establish the connection with the GPRS support node with the first version of GTP was successful and if the attempt to establish the connection with the GPRS support node with the first version of GTP was successful storing a first GTP version identifier in the GTP version cache table in association with the APN or APN IP address. If the attempt to establish a connection with the GPRS support node with the first version of GTP was not successful these embodiments include attempting to establish the connection with the GPRS support node with a second version of GTP determining if the attempt to establish a connection with the GPRS support node with the second version of GTP was successful and if the attempt to establish a connection with the GPRS support node with a second version of GTP was successful storing a second GTP version identifier in the GTP version cache table in association with the APN or APN IP address. For example the first version of GTP can be GTPV1 and the second version of GTP can be GTPV0. As suggested above the invention can be implemented within a Serving GPRS support node (SGSN). Such an SGSN might include a GTP version cache including a plurality of GTP version identifiers in association with a respective plurality of APNs or APN IP addresses and a respective plurality of context counts. Additionally the SGSN might include means for receiving a GTP tunnel setup request message associated with an APN or APN IP address means for attempting to establish a connection with the GPRS support node with a version of GTP associated with the previously stored GTP version identifier means for incrementing a context count when a context is successfully set up using an APN or APN IP address and a GTP version associated with a GTP version identifier stored in the GTP version cache in association with the APN or APN IP address and means for decrementing the context count when the context associated with the APN or APN IP address and the GTP version associated with the GTP version identifier stored in the GTP version cache in association with the APN or APN IP address is torn down. Some embodiments of such an SGSN include means for updating a respective timer value according to a passage of time means for starting the means for updating the respective timer value when a respective context count has a zero value and means for removing an association between a respective GTP version identifier and a respective APN or APN IP address when the respective timer value reaches a predetermined value. Some of these embodiments include means for stopping the means for updating the respective timer value before the respective timer value reaches the predetermined value if the respective context count is incremented above the zero value. Some embodiments include means for removing an association between a respective GTP version identifier and a respective APN or APN IP address if an attempt to establish a connection with the GPRS support node with a version of GTP associated with the respective GTP version identifier fails. Some embodiments include means for attempting to establish a connection with the GPRS support node with a trial version of GTP means for storing a trial GTP version identifier in the GTP version cache in association with the APN or APN IP address if the attempt to establish the connection with the GPRS support node with the trial version of GTP is successful. For example the trial version of GTP can be GTPV1 GTPV0 or some other version (i.e. GTPVn). The means for functions discussed above can take the form of networking and/or computer hardware and/or software configured and/or programmed to perform the described functions.'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tFplSjVksOJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming you have an existing DataFrame called df\n",
        "\n",
        "# Specify the file name and path where you want to save the CSV file\n",
        "file_name = 'llama.csv'\n",
        "file_path = '/content/drive/MyDrive/5731/assignment4-question3-data/' + file_name\n",
        "\n",
        "# Convert DataFrame to CSV with the specified file name\n",
        "df.to_csv(file_path, index=False)\n",
        "\n",
        "# Confirm the file has been created\n",
        "print(f\"CSV file saved as {file_name} at {file_path}\")\n"
      ],
      "metadata": {
        "id": "xjRkXDcZsx34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d403d8-4498-4edf-b778-e18716453f92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file saved as llama.csv at /content/drive/MyDrive/5731/assignment4-question3-data/llama.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/5731/assignment4-question3-data/llama.csv')"
      ],
      "metadata": {
        "id": "qTf8JgHaql91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiqlB3T0pl_2",
        "outputId": "f5fc9fb7-8eac-4d5d-cc1d-1319acf4dd58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Patent Number', 'Title', 'PDF Link', 'Abstract'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing and Training data"
      ],
      "metadata": {
        "id": "6Q4xWFDMwZgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "reformatted_df = df[['Patent Number', 'PDF Link', 'Abstract']].copy()\n",
        "\n",
        "reformatted_df.rename(columns={'Patent Number': 'patent_id', 'PDF Link': 'link'}, inplace=True)\n",
        "\n",
        "# Concatenate text from remaining columns into 'X', excluding 'Abstract' and 'Classifications'\n",
        "reformatted_df['X'] = df.drop(columns=['Patent Number', 'PDF Link', 'Abstract']).apply(lambda row: ' '.join(row), axis=1)\n",
        "\n",
        "# Assign 'Abstract' to 'Y_ref'\n",
        "reformatted_df['Y_ref'] = df['Abstract']\n",
        "\n",
        "# Drop unnecessary columns\n",
        "reformatted_df.drop(columns=['Abstract'], inplace=True)\n",
        "\n",
        "# Check the column names\n",
        "print(reformatted_df.columns)\n",
        "\n",
        "reformatted_df.columns\n",
        "\n",
        "# Split the data into train and test sets (80% train, 20% test)\n",
        "train_df, test_df = train_test_split(reformatted_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display the shapes of train and test sets\n",
        "print(\"Train set shape:\", train_df.shape)\n",
        "print(\"Test set shape:\", test_df.shape)\n",
        "\n",
        "# Save the train and test sets to CSV files\n",
        "\n",
        "file_name = 'Train.csv'\n",
        "file_path = '/content/drive/MyDrive/5731/assignment4-question3-data/' + file_name\n",
        "file_name = 'Test.csv'\n",
        "file_path = '/content/drive/MyDrive/5731/assignment4-question3-data/' + file_name\n",
        "# Confirm the files have been saved\n",
        "print(\"Train and test sets saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIRHvEepuuyN",
        "outputId": "fb150e17-d588-4794-b824-73ec05e0fd9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['patent_id', 'link', 'X', 'Y_ref'], dtype='object')\n",
            "Train set shape: (1304, 4)\n",
            "Test set shape: (326, 4)\n",
            "Train and test sets saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reformatted_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhXSwlyhzhST",
        "outputId": "09404efa-73d6-4bbc-9b65-919c3f91e765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['patent_id', 'link', 'X', 'Y_ref'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reformatted_df.to_csv('/content/drive/MyDrive/5731/dataset.csv')"
      ],
      "metadata": {
        "id": "eKplhOohymmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "73yOuU76u3o0",
        "outputId": "a2614f67-8146-44a5-81f8-f1ef4a5df95b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trl\n",
            "  Downloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/245.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/245.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl) (2.2.1+cu121)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from trl) (4.40.1)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.10/dist-packages (from trl) (1.25.2)\n",
            "Collecting accelerate (from trl)\n",
            "  Downloading accelerate-0.30.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets (from trl)\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tyro>=0.5.11 (from trl)\n",
            "  Downloading tyro-0.8.3-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.0/102.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->trl)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (4.66.2)\n",
            "Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.7.1)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->trl) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->trl)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (2.0.3)\n",
            "Collecting xxhash (from datasets->trl)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->trl)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.9.5)\n",
            "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers>=4.31.0->trl)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n",
            "Installing collected packages: xxhash, shtab, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, huggingface-hub, tyro, nvidia-cusolver-cu12, datasets, accelerate, trl\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed accelerate-0.30.0 datasets-2.19.0 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 shtab-1.7.1 trl-0.8.6 tyro-0.8.3 xxhash-3.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "huggingface_hub"
                ]
              },
              "id": "21de9b43785b407c89f8577a88fa0683"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetuning a PLM for summarization"
      ],
      "metadata": {
        "id": "C2qZgqCvxKVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = \"t5-small\"  # You can change this to other T5 variants like \"t5-base\", \"t5-large\", etc.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "UBqbfUIiDFFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Data Preparation\n",
        "# Split dataset into train and validation sets\n",
        "X_train, X_val, Y_ref_train, Y_ref_val = train_test_split(X, Y_ref, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Model Selection\n",
        "model_name = \"t5-base\"  # Choose the model architecture\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Step 3: Tokenization\n",
        "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
        "val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "# Step 4: Fine-tuning Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "train_dataset = CustomDataset(train_encodings, Y_ref_train)\n",
        "val_dataset = CustomDataset(val_encodings, Y_ref_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "\n",
        "# Step 5: Model Fine-tuning\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    # Evaluation on validation set\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "# Step 6: Evaluation\n",
        "# Generate summaries for the test set\n",
        "test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
        "test_dataset = CustomDataset(test_encodings, Y_ref_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "predictions = []\n",
        "for batch in test_loader:\n",
        "    input_ids = batch[\"input_ids\"].to(device)\n",
        "    attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=150, num_beams=2, early_stopping=True)\n",
        "        batch_predictions = [tokenizer.decode(ids, skip_special_tokens=True) for ids in output_ids]\n",
        "        predictions.extend(batch_predictions)\n",
        "\n",
        "# Add predictions to test set DataFrame\n",
        "test_df[\"Y_predict\"] = predictions\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "wnad0N6WFsag",
        "outputId": "c167bc30-d462-4ed2-d8f1-7724f306b038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-5c9d9d89e9a4>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Step 1: Data Preparation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Split dataset into train and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_ref_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_ref_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Step 2: Model Selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    }
  ]
}